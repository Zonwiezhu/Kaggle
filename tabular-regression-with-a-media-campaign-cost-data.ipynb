{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/zonwie/s3e11-beginner-decisiontree?scriptVersionId=125992770\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"03ebc82f-f31c-4573-81f7-e217957a2223","_cell_guid":"81af5c6a-e305-4a6e-9346-61ba3c2ec1dd","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:14:21.418132Z","iopub.execute_input":"2023-03-26T13:14:21.41934Z","iopub.status.idle":"2023-03-26T13:14:21.430786Z","shell.execute_reply.started":"2023-03-26T13:14:21.419252Z","shell.execute_reply":"2023-03-26T13:14:21.429333Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style = \"background-color: beige; border-radius: 30px 20px;\"> \n<h1 style = \"text-align:center;\">Summary</h1>\n</div>","metadata":{"_uuid":"24bb6829-4a67-45d3-8da6-2fb27609f94a","_cell_guid":"e597caed-1134-42d5-890a-4f827f53f61e","trusted":true}},{"cell_type":"markdown","source":"<p style = \"font-size: 20px; text-align:center;\">Food Mart is a convenience store in the US. They have 325 stores all located in the US. \"The task is to devise a Machine Learning Model that helps us predict the cost of media campaigns in the food markts on the basis of the features provided.\"\n<br>Submissions are scored on the root mean squared log error (RMSLE)</p>\n\n\nSource: https://www.kaggle.com/datasets/gauravduttakiit/media-campaign-cost-prediction","metadata":{"_uuid":"da6823d1-fab0-4723-b048-5068cc0d1e2d","_cell_guid":"a18fe26e-da10-46d0-b989-0c12f9a4924d","trusted":true}},{"cell_type":"markdown","source":"<div style = \"background-color: beige; border-radius: 30px 20px;\"> \n<h1 style = \"text-align:center;\">Data Exploring</h1>\n</div>","metadata":{"_uuid":"f2fa5056-7eb2-482a-a16b-9a4c0e92b265","_cell_guid":"8917aa5c-19e3-49f9-a9c7-29017c884f28","trusted":true}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/playground-series-s3e11/train.csv') #Import the dataset.\ndf.head() #Explore the five first data rows.","metadata":{"_uuid":"389699e6-ca6d-41db-b538-98d71788b7cf","_cell_guid":"07c9530b-67ec-406d-9f2d-ff422e3acba6","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T14:36:38.389085Z","iopub.execute_input":"2023-03-26T14:36:38.38953Z","iopub.status.idle":"2023-03-26T14:36:38.929737Z","shell.execute_reply.started":"2023-03-26T14:36:38.389489Z","shell.execute_reply":"2023-03-26T14:36:38.928375Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape #Let's see what the shape is.","metadata":{"_uuid":"5a4690f0-7d8d-4c36-818c-bb315ba3de02","_cell_guid":"b0dc61c5-a0f9-46dd-aa0c-c085158fd267","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T14:36:43.148318Z","iopub.execute_input":"2023-03-26T14:36:43.14967Z","iopub.status.idle":"2023-03-26T14:36:43.156754Z","shell.execute_reply.started":"2023-03-26T14:36:43.149621Z","shell.execute_reply":"2023-03-26T14:36:43.155366Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes #Check the datatypes in dataframe.","metadata":{"_uuid":"a4fbffa6-5e90-4c6b-852c-2f01bf6a6eb9","_cell_guid":"01ad2633-8634-4ec1-a578-9732d8acd828","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T14:36:50.572813Z","iopub.execute_input":"2023-03-26T14:36:50.57327Z","iopub.status.idle":"2023-03-26T14:36:50.583268Z","shell.execute_reply.started":"2023-03-26T14:36:50.573223Z","shell.execute_reply":"2023-03-26T14:36:50.58196Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe() #Check the dataframe summary.","metadata":{"_uuid":"ac9b2271-69c0-45b7-95cb-9b3e28ac45ae","_cell_guid":"a075f905-38b1-47bf-8ba1-389e5c3a12df","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T14:36:57.783579Z","iopub.execute_input":"2023-03-26T14:36:57.784003Z","iopub.status.idle":"2023-03-26T14:36:58.111261Z","shell.execute_reply.started":"2023-03-26T14:36:57.783969Z","shell.execute_reply":"2023-03-26T14:36:58.109808Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p style = \"font-size: 20px; text-align:center;\">Let's look at each columns what it all does (Dataset description):</p>\n\n- Store_sales (in millions) = Store sales in millions dollars --> No Nan/Null values, mean: 6.337, min: 0.51 and max: 22.92. Maybe outliers in this phase! (Continuous variable).\n\n- Unit_sales (in millions) = Unit_sales in millions in stores --> No Nan/Null values, mean: 3.0439, min 1 and max: 6. There is no outliers! (Categorical variable).\n\n- Total_children = Childeren in home --> No Nan/Null value, mean: 2.4564, min: 0, max: 5. There is no outliers! (Categorical variable).\n\n- avg_cars_at home(approx) = Avarage cars at home --> No Nan/Null value, mean: 0.689390, min: 0, max: 4. There is no outliers! (Categorical variable).\n\n- Gross_weight = Gross weight of items --> No Nan/Null value, mean: 13.822, min: 6, max: 21.9. There is no outliers! (Continous variable).\n\n- Recyclable_package = Food items is in recylable package --> No Nan/Null value, mean 0.5--> No Nan/Null value681, min: 0, max: 1. There is no outliers! (Categorical variable).\n\n- Low_fat = Food items is low fat --> No Nan/Null value, mean: 0.327797, min: 0, max: 1. There is no outliers! (Categorical variable).\n\n- Units_per_case = Case units available in each store shelves --> No Nan/Null value, mean: 18.97, min: 1, max: 36. There is no outliers! (Categorical variable).\n\n- Store_sqft = Store area available in SQFT --> No Nan/Null value, mean: 28180.333442, min: 5968,874074, max: 39696. There is no outliers! (Continous variable).\n\n- Coffee_bar = Coffee bar available in store --> No Nan/Null value, mean: 0.5648, min: 0, max: 1. There is no outliers! (Categorical variable).\n\n- Video_store = Video store available in store --> No Nan/Null value, mean: 0.27739, min: 0, max: 1. There is no outliers! (Categorical variable).\n\n- Salad_bar = Salad bar available in store --> No Nan/Null value, mean: 0.5048, min: 0, max: 1. There is no outliers! (Categorical variable).\n\n- Prepared_food = Food prepared available in store --> No Nan/Null value, mean: 0.5408, min: 0, max: 1. There is no outliers! (Categorical variable).\n\n- Florist = Flower shelves available in store --> No Nan/Null value, mean: 0.50319, min:0, max: 1. There is no outliers! (Categorical variable).\n\n<p style = \"font-size: 20px; text-align:center;\">Everything above are features to make a prediction (input data). The predicted value is cost (continous variable) (output data).</p>","metadata":{"_uuid":"82f95afe-0d2b-4549-af36-40eab956751f","_cell_guid":"722abcc2-dcab-4637-b5f8-60fc4fad9844","trusted":true}},{"cell_type":"markdown","source":"<div style = \"background-color: beige; border-radius: 30px 20px;\"> \n<h1 style = \"text-align:center;\">Machine Learning</h1>\n</div>","metadata":{"_uuid":"7e5019ce-7548-45e8-b185-6e7016aa8e5b","_cell_guid":"cf202df0-087a-4810-87da-f5f25f9b0f0c","trusted":true}},{"cell_type":"markdown","source":"<p style = \"font-size: 20px; text-align:center;\">For choosing which Machine Learning model we gonna use. We need first choose the three major types of Machine Learning:</p>\n\n- Unsupervised machine learning = based on the idea that a machine can learn without any guidance from humans. It uses unlabeled data! Examples: Clustering or Dimenisionality Reduction.\n\n- Supervised machine learning = flexible, comprehensive and covers a lot of the common machine learning tasks that are in high demand today. It uses labeled data! Examples: Regression, Classification or Forecasting.\n\n- Reinforcement algorithm = Completly different. It trains within an environment with a set of rules and a defined goal.\n\nSource: https://towardsdatascience.com/types-of-machine-learning-algorithms-you-should-know-953a08248861","metadata":{"_uuid":"cd864ddf-208f-492e-a99e-faf432e64118","_cell_guid":"c2ddccc1-5f59-4115-a8b0-99fbe8fb3959","trusted":true}},{"cell_type":"markdown","source":"\n<p style = \"font-size: 20px; text-align:center;\">We have labels (output data) on the dataset that we can use for checking our errors. We gonna use supervised machine learning!</p>","metadata":{"_uuid":"ad0121f1-4122-4102-aa4d-6a4d877a97cd","_cell_guid":"54282e8a-826c-4b04-ad0b-60d119727e50","trusted":true}},{"cell_type":"markdown","source":"<div style = \"background-color: bisque; border-radius: 30px 20px;\"> \n<h2 style = \"text-align:center;\">Supervised machine learning algorithm that are popular:</h2>\n</div>","metadata":{"_uuid":"d6b6214b-6e4d-4c06-85b1-c777d70dbf93","_cell_guid":"252d4199-c841-4d99-87d7-39bb6112914b","trusted":true}},{"cell_type":"markdown","source":"- Linear regression (Predicting continous values)\n\n- Logistic regression (Predicting categorical values)\n\n- Decision tree (Predicting categorigcal and continous variable)\n\n- SVM algorithm (Predicting categorical and continous variable)\n\n- Naive Bayes algorithm (Predicting categorical and contnous variable)\n\n- KNN algorithm (Predicting categorical and continous variable)\n\n- K - means (Prediciting categorical and continous variable)\n\n- Random forest algorithms (Predicting categorical and continous variable)\n\n- Dimensionality reduction algorithms (Not for predicting just for reducing)\n\n- Gradient boosting algorithm (Predicting categorical and continous variable)\n\nSource: https://www.simplilearn.com/10-algorithms-machine-learning-engineers-need-to-know-article","metadata":{"_uuid":"65c78d45-8841-4330-a32c-2c1ecff0a9a1","_cell_guid":"2a87482a-7467-477f-a587-a86781261461","execution":{"iopub.status.busy":"2023-03-24T17:34:26.656288Z","iopub.execute_input":"2023-03-24T17:34:26.65673Z","iopub.status.idle":"2023-03-24T17:34:26.666185Z","shell.execute_reply.started":"2023-03-24T17:34:26.656695Z","shell.execute_reply":"2023-03-24T17:34:26.664709Z"},"trusted":true}},{"cell_type":"markdown","source":"<div style = \"background-color: bisque; border-radius: 30px 20px;\"> \n<h2 style = \"text-align:center;\">Supervised choosing:</h2>\n</div>","metadata":{"_uuid":"76c4251d-7a09-4bcc-908c-02a54dcce7d8","_cell_guid":"55e56664-e996-46a5-88d0-6aabd8503637","trusted":true}},{"cell_type":"markdown","source":"<p style = \"font-size: 20px; text-align:center;\">Supervised machine learning that we can use: <br>Linear regression, Decision tree, SVM, Naive Bayes, KNN, K-means, Random forest and Gradient boosting.</p>\n\n- Linear regression assumption:\n    1. Linearity: Linear regression assumes that there is a linear relationship between the preditor variables and the target variable. \n    2. Independence: Linear regression assumes that the observations are independent of each other, and that there is no autocorrelation between the error terms.\n    3. Homoscedasticity: Linear regression assumes that the error terms have constant variance across all levels of the predictor cariables.\n    4. Normality: Linear regression assumes that the error terms are normally distributed.\n    5. No multicollinearuty: Linear regression assumes that there is no high correlation between the predictor variables.\n\n- Decision tree will be regression tree assumption: Unlike linear regression, decision trees do not rely on strong assumptions about the relationship between the predictor variables and the target variables.\n\n- SVM assumption:\n    1. Linear separability: The most basic form of SVM assumes that the data is linearly separable, which means that there exists a hyperplane that can perfectly separate the data points of different classes or predict the target variable with zero error.\n    2. Margins: SVMs aim to find the hyperplane that maximizes the margin between the data points of different classes or the target variable and the hyperplane. The margin is the distance between the hyperplane and the closest data points of each class or the target variable.\n    3. Kernel trick: If the data is not linearly separable, SVMs can use a kernel function to map the data to a higher-dimensional feature space where it becomes separable. This allows SVMs to capture nonlinear patterns in the data and to find a more flexible decision boundary.\n    4. Outliers: SVMs are robust to outliers because they focus on the points that are closest to the decision boundary and ignore the points that are far away from it.\n    \n\n- Naive Bayes assumption:\n    1. Independence: Naive Bayes assumes that the predictor variables are independent of each other given the class variable. This assumption is often not true in real-world datasets, but Naive Bayes can still perform well even if the independence assumption is violated.\n    2. Normal distribution: Naive Bayes assumes that the predictor variables are normally distributed. If the variables are not normally distributed, they can be transformed to follow a normal distribution.\n    3. Large sample size: Naive Bayes performs well when the sample size is large enough to estimate the probabilities accurately. If the sample size is too small, the estimates may be unreliable and the algorithm may overfit the data.\n    4. Feature relevance: Naive Bayes assumes that the predictor variables are relevant to the class variable. If a predictor variable has no relationship with the class variable, Naive Bayes may still use it in the model and decrease the accuracy.\n    5. Class balance: Naive Bayes works well when the classes are balanced or when the rare classes have enough examples to estimate their probabilities accurately. If the rare classes are underrepresented, Naive Bayes may perform poorly.\n\n\n- KNN assumption: Doen't make any explicit assumptions about the underlying data distribution!\n\n\n- Random forest assumption: \n    1. Independence: Each decision tree in the Random Forest should be independent of each other.\n    2. Feature relevance: Random Forest assumes that the predictor variables are relevant to the response variable.\n    3. Low correlation between trees: The decision trees in the Random Forest should have low correlation with each other. This means that each decision tree should make different splits and predictions.\n    4. Large sample size: Random Forest performs well when the sample size is large enough to estimate the probabilities accurately.\n    5. Variable importance: Random Forest can identify the most important predictor variables for the response variable.\n\n\n- K-means assumption:\n    1. Number of clusters: K-Means assumes that the number of clusters is known in advance. This means that the user needs to specify the number of clusters that the algorithm should find in the data.\n    2. Data distribution: K-Means assumes that the data is normally distributed and has a spherical shape. This means that the distance metric used in the algorithm (usually Euclidean distance) is appropriate for the data.\n    3. Cluster size: K-Means assumes that the size of the clusters is roughly equal. This means that the algorithm may not work well if there are clusters of different sizes.\n    4. Data scaling: K-Means is sensitive to the scale and range of the data features. If the features have different scales or ranges, normalization or standardization can be used to ensure that all features contribute equally to the clustering.\n    5. Random initialization: K-Means uses random initialization of cluster centroids, which means that the algorithm can converge to different solutions depending on the initial conditions. Running the algorithm multiple times with different initializations can help to find a better solution.\n\n\n- Gradient boosting assumption:\n    1. Data quality: Gradient Boosting performs well when the data is of high quality and free from missing values and outliers.\n\n\nsource: https://www.kdnuggets.com/2021/02/machine-learning-assumptions.html","metadata":{"_uuid":"a489959d-1ad2-4cde-a337-2ee6f59cd576","_cell_guid":"03acbb63-24d6-4e1a-a7cb-02c9f9b01f88","trusted":true}},{"cell_type":"markdown","source":"<p style = \"font-size: 20px; text-align:center;\">Sooooo we are gonna use one algorithm Decision tree, because there is no explicit assumptions and KNN processing time is to long!</p>","metadata":{"_uuid":"d21959e1-7d79-4ba1-bfd7-b57d5f698cbf","_cell_guid":"5b600c6d-d61d-4f73-87f3-443c4cacf47d","trusted":true}},{"cell_type":"markdown","source":"<div style = \"background-color: beige; border-radius: 30px 20px;\"> \n<h1 style = \"text-align:center;\">Data preparation</h1>\n</div>","metadata":{"_uuid":"c731d824-9a7b-4277-9d08-e90e66281eac","_cell_guid":"712ca6c0-18f2-4729-91c8-615512d5ce41","trusted":true}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split #Split arrays or matrices into random train and test subsets.\nX = df.drop(['cost', 'id'], axis = 1) #Remove the cost column (it's now only have the predictor variable).\ny = df['cost'] #Select the target variable.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.02) #Split the data in 98% trainingsset and 2% testset.","metadata":{"_uuid":"7dcb82f7-d007-4b79-b980-19d1e14a30a2","_cell_guid":"cfc56acf-dc91-498c-b293-eb2d17cab938","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:14:33.432649Z","iopub.execute_input":"2023-03-26T13:14:33.433075Z","iopub.status.idle":"2023-03-26T13:14:33.517128Z","shell.execute_reply.started":"2023-03-26T13:14:33.433039Z","shell.execute_reply":"2023-03-26T13:14:33.516012Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style = \"background-color: beige; border-radius: 30px 20px;\"> \n<h1 style = \"text-align:center;\">Decision tree</h1>\n</div>","metadata":{"_uuid":"ce4d0b1d-f986-4143-a1f6-e66f103fd913","_cell_guid":"81f08be1-076a-42f2-950d-ed7d716e41af","trusted":true}},{"cell_type":"markdown","source":"Decision Tree Regression have an advantage that it is easy to understand, lesser data cleaning is required, non-linearity does not affect the model's performance and the number of hyper-parameters to be tuned is almost null. However, it may have an over-fitting problem!\n\n<p style = \"font-size: 20px; text-align:center;\">Overfitting is a common explanation for the poor performace of a predictive model. So we need to look out how depth our tree need to be!</p>","metadata":{"_uuid":"81713fcb-96fc-44dd-8251-099c94e2ef05","_cell_guid":"791987af-57c8-4e4a-9613-4e0919a59ce4","trusted":true}},{"cell_type":"code","source":"# Import the necessary modules and libraries for decision tree.\nfrom matplotlib import pyplot\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn import tree \n\n# Define the tree depths to evaluate:\ndepths = [ i for i in range(1, 21)]\n\n# Empty lists:\ntrain_score = []\ntest_score = []\n\n# For loop for all tree depths.\nfor i in depths:\n    model = tree.DecisionTreeRegressor(max_depth= i) #Run the model.\n    model = model.fit(X_train, y_train) #Train the model.\n    \n    y_pred_train = model.predict(X_train) #With that model lets predict.\n    mean_squared_log_train = mean_squared_log_error(y_train, y_pred_train) #Calculate the MSLE.\n    train_score.append(mean_squared_log_train) #Append to the list.\n    \n    y_pred_test = model.predict(X_test)\n    mean_squared_log_test = mean_squared_log_error(y_test, y_pred_test)\n    test_score.append(mean_squared_log_test)\n    \n    print('>%d, train: %.3f, test: %.3f' % (i, mean_squared_log_train, mean_squared_log_test))","metadata":{"_uuid":"b8f53b5f-3495-4626-9089-b311a93ae2b7","_cell_guid":"4fdada25-6aca-49fa-887e-3ce6abeb1932","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:14:44.878474Z","iopub.execute_input":"2023-03-26T13:14:44.878889Z","iopub.status.idle":"2023-03-26T13:15:13.592351Z","shell.execute_reply.started":"2023-03-26T13:14:44.878854Z","shell.execute_reply":"2023-03-26T13:15:13.591104Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pyplot.plot(depths, train_score, '-o', label='Train') #Plot the trainscore.\npyplot.plot(depths, test_score, '-o', label='Test') #Plot the testscore.\npyplot.legend()\npyplot.show() #Show the plot.","metadata":{"_uuid":"36314fac-e021-412f-958f-9544b36ea5bb","_cell_guid":"18be6a5e-b7fa-4d7f-b3b5-6363b54fc23e","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:15:14.988685Z","iopub.execute_input":"2023-03-26T13:15:14.989672Z","iopub.status.idle":"2023-03-26T13:15:15.236701Z","shell.execute_reply.started":"2023-03-26T13:15:14.989628Z","shell.execute_reply":"2023-03-26T13:15:15.235179Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://)<p style = \"font-size: 20px; text-align:center;\">We can see that the MSLE on the test set improves with tree depth until a depth of about nive, after this it's begin to get worse and that is overfitting! So max-depth will be nine!<p>","metadata":{"_uuid":"f57da83d-412a-471a-a8ae-0d1005f397d8","_cell_guid":"ec78daea-f313-4f44-94d1-8c55dfe1d83c","trusted":true}},{"cell_type":"code","source":"model = tree.DecisionTreeRegressor(max_depth= 9) #Run the model with depth 9.\nmodel = model.fit(X_train, y_train) #Train the model.","metadata":{"_uuid":"2c977081-b13f-4c63-aa2c-a96143443c48","_cell_guid":"adbf831d-e73f-45fd-a849-901b0d03de21","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:15:20.093711Z","iopub.execute_input":"2023-03-26T13:15:20.094148Z","iopub.status.idle":"2023-03-26T13:15:21.365387Z","shell.execute_reply.started":"2023-03-26T13:15:20.094111Z","shell.execute_reply":"2023-03-26T13:15:21.364035Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/playground-series-s3e11/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s3e11/sample_submission.csv')\n\ntest = test.drop('id', axis = 1) #Remove the cost column (it's now only have the predictor variable).\n\nResult = model.predict(test) #Run the test dataset with the model we build.\n\nsubmission_id = submission['id'] #Select the id!\n\nprediction = pd.DataFrame(Result, columns=['cost'], index=submission_id).to_csv('predictionwithDecisionTree.csv') #Put in the csv file!","metadata":{"_uuid":"af168eac-987c-486b-9379-bcb2f09c7a71","_cell_guid":"021e68b8-d6b7-4b44-b708-3c8bb021e8d3","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:15:24.876874Z","iopub.execute_input":"2023-03-26T13:15:24.87728Z","iopub.status.idle":"2023-03-26T13:15:25.839352Z","shell.execute_reply.started":"2023-03-26T13:15:24.877242Z","shell.execute_reply":"2023-03-26T13:15:25.838117Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size: 20px; text-align:center;\">This give us the MSLE score: 0.30597. Place - 424.<p>","metadata":{"_uuid":"34f71a34-6342-47f1-b38e-3acadaf0ef46","_cell_guid":"33c1c846-5b8c-4f0b-80cd-2564a05783e4","trusted":true}},{"cell_type":"markdown","source":"<div style = \"background-color: beige; border-radius: 30px 20px;\"> \n<h1 style = \"text-align:center;\">Feature selection</h1>\n</div>","metadata":{"_uuid":"c121b660-483f-4da0-ba3c-81e2ea644aef","_cell_guid":"5f9eaaaf-5cf2-4731-943e-4225ca73835d","trusted":true}},{"cell_type":"markdown","source":"<p style = \"font-size: 20px; text-align:center;\">Can we perform better than this? Hell Yeah maybe! </p>\n\nLet's try feature selection. Feature selection is the process of selecting a subset of the most relevant features of variables from a larger set of features in order to improve the performance of a machine learning model. (Reduce overfitting, Improve model performance, reduce computational complexity).\n\n- Filter methods:\n\nFor a dataset with both continous and categorical input features and continous output we can use correlation-based feature selection or mutual information.\n\nWe gonna use correlation-based feature selection:","metadata":{"_uuid":"ef41f096-6fec-4644-a6f7-f1a11f2bdce8","_cell_guid":"4c9e89ec-7dc1-4c4c-83e6-c85eac78de17","trusted":true}},{"cell_type":"code","source":"#Import the necessary modules and libraries.\nimport seaborn as sns\n\npyplot.figure(figsize=(20,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=pyplot.cm.Reds)\npyplot.show()","metadata":{"_uuid":"da98d612-4e53-4498-bb73-497ed6dd77b2","_cell_guid":"1d98558a-47a3-45ad-a124-0772c19cce79","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:15:30.808688Z","iopub.execute_input":"2023-03-26T13:15:30.809141Z","iopub.status.idle":"2023-03-26T13:15:33.263382Z","shell.execute_reply.started":"2023-03-26T13:15:30.8091Z","shell.execute_reply":"2023-03-26T13:15:33.262073Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will not select features that above 0.6 correlation, because feature with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, I can drop on of the two features:\n\n- video store ==> high correlated with salad_bar, prepared_food and florist. So I would recommand to use one of the four features!\n\n<p style = \"font-size: 20px; text-align:center;\">I wil choose salad_bar!<p>","metadata":{"_uuid":"b88a52f7-d141-4773-9804-c5275a51d133","_cell_guid":"d442a55b-93b0-48e3-9d7e-0c71fe8c7f7a","trusted":true}},{"cell_type":"code","source":"#Remove three features: video_store, prepared_food and florist.\nX_train = X_train.drop(['video_store', 'prepared_food', 'florist'], axis = 1) \nX_test = X_test.drop(['video_store', 'prepared_food', 'florist'], axis = 1) \ntest = test.drop(['video_store', 'prepared_food', 'florist'], axis = 1)","metadata":{"_uuid":"de837d18-acd7-42ec-a953-b26037e9aec9","_cell_guid":"896dc91e-b7f1-4c02-a877-e954f95e93b6","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:42:00.190973Z","iopub.execute_input":"2023-03-26T13:42:00.191446Z","iopub.status.idle":"2023-03-26T13:42:00.220163Z","shell.execute_reply.started":"2023-03-26T13:42:00.191405Z","shell.execute_reply":"2023-03-26T13:42:00.218681Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define the tree depths to evaluate:\ndepths = [ i for i in range(1, 21)]\n\n#Empty list:\ntrain_score = []\ntest_score = []\n\n#For loop for all tree depths.\nfor i in depths:\n    model = tree.DecisionTreeRegressor(max_depth= i)  #Run the model.\n    model = model.fit(X_train, y_train) #Train the model.\n    \n    y_pred_train = model.predict(X_train) #With that model lets predict.\n    mean_squared_log_train = mean_squared_log_error(y_train, y_pred_train) #Calculate the MSLE.\n    train_score.append(mean_squared_log_train) #Append to the list.\n    \n    y_pred_test = model.predict(X_test)\n    mean_squared_log_test = mean_squared_log_error(y_test, y_pred_test)\n    test_score.append(mean_squared_log_test)\n    \n    print('>%d, train: %.3f, test: %.3f' % (i, mean_squared_log_train, mean_squared_log_test))","metadata":{"_uuid":"cafc9d9e-33bc-4317-a36c-0a82db817919","_cell_guid":"ab85ad9a-0baf-447f-920c-ed5ff39a86de","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:42:13.14227Z","iopub.execute_input":"2023-03-26T13:42:13.142724Z","iopub.status.idle":"2023-03-26T13:42:40.479005Z","shell.execute_reply.started":"2023-03-26T13:42:13.142683Z","shell.execute_reply":"2023-03-26T13:42:40.477744Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pyplot.plot(depths, train_score, '-o', label='Train') #Plot the trainscore.\npyplot.plot(depths, test_score, '-o', label='Test') #Plot the testscore.\npyplot.legend()\npyplot.show() #Show the plot.","metadata":{"_uuid":"5b8f85cc-345e-44a9-a2a1-c63634d19007","_cell_guid":"2ea10c1e-bc1b-4bae-8d77-de235d1139fb","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:43:06.696113Z","iopub.execute_input":"2023-03-26T13:43:06.697407Z","iopub.status.idle":"2023-03-26T13:43:06.936853Z","shell.execute_reply.started":"2023-03-26T13:43:06.697339Z","shell.execute_reply":"2023-03-26T13:43:06.935441Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size: 20px; text-align:center;\">We can see that the MSLE on the test set improves with tree depth until a depth of about eleven, after this it's begin to get worse and that is overfitting! So max-depth will be eleven!<p>","metadata":{"_uuid":"54dc081c-6b5f-4b63-8b6a-16f436e8abde","_cell_guid":"83ccb7c0-875e-405b-ae1a-dc8f6469fe39","trusted":true}},{"cell_type":"code","source":"model = tree.DecisionTreeRegressor(max_depth= 11) #Run the model with depth 9.\nmodel = model.fit(X_train, y_train) #Train the model.","metadata":{"_uuid":"61217965-98fc-454f-8bae-6c8b147a7b4a","_cell_guid":"58ae6e3a-436a-42e5-ba88-776443da7a9b","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:44:12.560054Z","iopub.execute_input":"2023-03-26T13:44:12.560497Z","iopub.status.idle":"2023-03-26T13:44:13.967032Z","shell.execute_reply.started":"2023-03-26T13:44:12.560459Z","shell.execute_reply":"2023-03-26T13:44:13.965815Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Result = model.predict(test) #Run the test dataset with the model we build.\n\nsubmission_id = submission['id'] #Select the id!\n\nprediction = pd.DataFrame(Result, columns=['cost'], index=submission_id).to_csv('submission.csv') #Put in the CSV!","metadata":{"_uuid":"91d363ef-508c-42e8-8582-f444a638a5fc","_cell_guid":"ead63c90-023b-4a56-8bcd-ce5125dc4402","collapsed":false,"execution":{"iopub.status.busy":"2023-03-26T13:44:28.960164Z","iopub.execute_input":"2023-03-26T13:44:28.960596Z","iopub.status.idle":"2023-03-26T13:44:29.574604Z","shell.execute_reply.started":"2023-03-26T13:44:28.960558Z","shell.execute_reply":"2023-03-26T13:44:29.572996Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style = \"font-size: 20px; text-align:center;\">This give us the MSLE score: 0.30221. Place - 393.<p>","metadata":{"_uuid":"f8c838b3-413f-4897-8401-1c7dc2bd5901","_cell_guid":"dc73f5f9-29ed-4544-8cc4-43c1405d69c1","trusted":true}},{"cell_type":"markdown","source":"<p style = \"font-size: 20px; text-align:center;\"> The next time i wanna try other machine learning algorithm! Thanks for reading!<p>","metadata":{"_uuid":"f47ee076-b86f-497e-b7ee-16b2f01eef5d","_cell_guid":"1d825f7a-a66d-490b-8df8-7a2fdc7b45bf","trusted":true}}]}